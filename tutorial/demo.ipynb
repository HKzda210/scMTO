{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16300a7c",
   "metadata": {},
   "source": [
    "In this demo, we will perform scMTO cluster analysis using the [Qx_Spleen](https://drive.google.com/drive/folders/1BIZxZNbouPtGf_cyu7vM44G5EcbxECeu) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03763f54",
   "metadata": {},
   "source": [
    "## Import python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf0596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from scMTO.preprocess import prepro, normalize\n",
    "from scMTO.model import scMTO\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab2aff",
   "metadata": {},
   "source": [
    "## Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d461e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: Quake_10x_Spleen\n"
     ]
    }
   ],
   "source": [
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled =  True\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='train', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--name', type=str, default='Quake_10x_Spleen', help=\"name of scRNA-seq dataset\")\n",
    "parser.add_argument('--pre_lr', type=float, default=4e-4, help=\"learning rate of pre-training\")\n",
    "parser.add_argument('--lr', type=float, default=1e-5, help=\"learning rate of formal-training\")\n",
    "parser.add_argument('--pre_epoch', type=int, default=250, help=\"epoch numbers of pre-training\")\n",
    "parser.add_argument('--train_epoch', type=int, default=500, help=\"epoch numbers of formal-training\")\n",
    "parser.add_argument('--latent_dim', default=32, type=int, help=\"dimension of latent space\")\n",
    "parser.add_argument('--device', type=int, default=3)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "args.cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(args.device)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "file_path = \"../data/\" + args.name + \"/data.h5\"\n",
    "print(\"dataset: {}\".format(args.name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c55f0",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ea56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = prepro(file_path)\n",
    "x = np.ceil(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6511f",
   "metadata": {},
   "source": [
    "## Single-cell multi-view feature space construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a56e9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-View 1 (normalized, 2000HVGs)\n",
    "adata1 = sc.AnnData(x)\n",
    "adata1.obs['Group'] = y\n",
    "adata1 = normalize(adata1, copy=True, highly_genes=2000, size_factors=True, normalize_input=True, logtrans_input=True)\n",
    "\n",
    "# Multi-View 2 (normalized, 500HVGs)\n",
    "adata2 = sc.AnnData(x)\n",
    "adata2.obs['Group'] = y   \n",
    "adata2 = normalize(adata2, copy=True, highly_genes=500, size_factors=True, normalize_input=True, logtrans_input=True)\n",
    "\n",
    "# Multi-View 3 (sparse topic patterns, 2000HVGs) \n",
    "highly_genes_index = [int(gene_idx) for gene_idx in list(adata1.var.index)]\n",
    "raw_data = np.ceil(adata1.raw.X[:, highly_genes_index]).astype(int)\n",
    "\n",
    "count = [adata1.X, adata2.X]\n",
    "args.n_clusters = int(max(y) - min(y) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cb49e",
   "metadata": {},
   "source": [
    "## Initialize model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de15895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the cell graph...\n",
      "Topic modeling...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model \n",
    "model = scMTO(n_z=args.latent_dim, n_clusters=args.n_clusters, x_raw=raw_data, device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d0834",
   "metadata": {},
   "source": [
    "## Pre-training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99bfbdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scMTO pretraining...\n",
      "0: Pretraining Loss:0.395810\n",
      "1: Pretraining Loss:0.386074\n",
      "2: Pretraining Loss:0.378549\n",
      "3: Pretraining Loss:0.370711\n",
      "4: Pretraining Loss:0.360898\n",
      "5: Pretraining Loss:0.348554\n",
      "6: Pretraining Loss:0.333225\n",
      "7: Pretraining Loss:0.315368\n",
      "8: Pretraining Loss:0.295715\n",
      "9: Pretraining Loss:0.274909\n",
      "10: Pretraining Loss:0.254963\n",
      "11: Pretraining Loss:0.236771\n",
      "12: Pretraining Loss:0.221970\n",
      "13: Pretraining Loss:0.210635\n",
      "14: Pretraining Loss:0.202552\n",
      "15: Pretraining Loss:0.197194\n",
      "16: Pretraining Loss:0.193612\n",
      "17: Pretraining Loss:0.190770\n",
      "18: Pretraining Loss:0.187992\n",
      "19: Pretraining Loss:0.185721\n",
      "20: Pretraining Loss:0.183317\n",
      "21: Pretraining Loss:0.181183\n",
      "22: Pretraining Loss:0.179022\n",
      "23: Pretraining Loss:0.177127\n",
      "24: Pretraining Loss:0.175174\n",
      "25: Pretraining Loss:0.173506\n",
      "26: Pretraining Loss:0.171849\n",
      "27: Pretraining Loss:0.170250\n",
      "28: Pretraining Loss:0.168920\n",
      "29: Pretraining Loss:0.167585\n",
      "30: Pretraining Loss:0.166134\n",
      "31: Pretraining Loss:0.164913\n",
      "32: Pretraining Loss:0.164002\n",
      "33: Pretraining Loss:0.163213\n",
      "34: Pretraining Loss:0.162294\n",
      "35: Pretraining Loss:0.161647\n",
      "36: Pretraining Loss:0.160810\n",
      "37: Pretraining Loss:0.160219\n",
      "38: Pretraining Loss:0.159523\n",
      "39: Pretraining Loss:0.158853\n",
      "40: Pretraining Loss:0.158335\n",
      "41: Pretraining Loss:0.157952\n",
      "42: Pretraining Loss:0.157426\n",
      "43: Pretraining Loss:0.156959\n",
      "44: Pretraining Loss:0.156570\n",
      "45: Pretraining Loss:0.156189\n",
      "46: Pretraining Loss:0.155831\n",
      "47: Pretraining Loss:0.155415\n",
      "48: Pretraining Loss:0.155260\n",
      "49: Pretraining Loss:0.154887\n",
      "50: Pretraining Loss:0.154603\n",
      "51: Pretraining Loss:0.154165\n",
      "52: Pretraining Loss:0.153938\n",
      "53: Pretraining Loss:0.153656\n",
      "54: Pretraining Loss:0.153417\n",
      "55: Pretraining Loss:0.153291\n",
      "56: Pretraining Loss:0.153251\n",
      "57: Pretraining Loss:0.152782\n",
      "58: Pretraining Loss:0.152546\n",
      "59: Pretraining Loss:0.152338\n",
      "60: Pretraining Loss:0.152160\n",
      "61: Pretraining Loss:0.152001\n",
      "62: Pretraining Loss:0.151753\n",
      "63: Pretraining Loss:0.151488\n",
      "64: Pretraining Loss:0.151295\n",
      "65: Pretraining Loss:0.151156\n",
      "66: Pretraining Loss:0.150939\n",
      "67: Pretraining Loss:0.150898\n",
      "68: Pretraining Loss:0.150572\n",
      "69: Pretraining Loss:0.150505\n",
      "70: Pretraining Loss:0.150200\n",
      "71: Pretraining Loss:0.149954\n",
      "72: Pretraining Loss:0.149806\n",
      "73: Pretraining Loss:0.149596\n",
      "74: Pretraining Loss:0.149381\n",
      "75: Pretraining Loss:0.149324\n",
      "76: Pretraining Loss:0.149053\n",
      "77: Pretraining Loss:0.148917\n",
      "78: Pretraining Loss:0.148759\n",
      "79: Pretraining Loss:0.148557\n",
      "80: Pretraining Loss:0.148385\n",
      "81: Pretraining Loss:0.148251\n",
      "82: Pretraining Loss:0.148145\n",
      "83: Pretraining Loss:0.147881\n",
      "84: Pretraining Loss:0.147690\n",
      "85: Pretraining Loss:0.147490\n",
      "86: Pretraining Loss:0.147316\n",
      "87: Pretraining Loss:0.147170\n",
      "88: Pretraining Loss:0.147092\n",
      "89: Pretraining Loss:0.146837\n",
      "90: Pretraining Loss:0.146710\n",
      "91: Pretraining Loss:0.146603\n",
      "92: Pretraining Loss:0.146470\n",
      "93: Pretraining Loss:0.146265\n",
      "94: Pretraining Loss:0.146187\n",
      "95: Pretraining Loss:0.145937\n",
      "96: Pretraining Loss:0.145814\n",
      "97: Pretraining Loss:0.145611\n",
      "98: Pretraining Loss:0.145487\n",
      "99: Pretraining Loss:0.145486\n",
      "100: Pretraining Loss:0.145294\n",
      "101: Pretraining Loss:0.145151\n",
      "102: Pretraining Loss:0.144984\n",
      "103: Pretraining Loss:0.144827\n",
      "104: Pretraining Loss:0.144663\n",
      "105: Pretraining Loss:0.144642\n",
      "106: Pretraining Loss:0.144440\n",
      "107: Pretraining Loss:0.144334\n",
      "108: Pretraining Loss:0.144164\n",
      "109: Pretraining Loss:0.144051\n",
      "110: Pretraining Loss:0.143896\n",
      "111: Pretraining Loss:0.143835\n",
      "112: Pretraining Loss:0.143689\n",
      "113: Pretraining Loss:0.143627\n",
      "114: Pretraining Loss:0.143481\n",
      "115: Pretraining Loss:0.143362\n",
      "116: Pretraining Loss:0.143192\n",
      "117: Pretraining Loss:0.143163\n",
      "118: Pretraining Loss:0.143030\n",
      "119: Pretraining Loss:0.142873\n",
      "120: Pretraining Loss:0.142778\n",
      "121: Pretraining Loss:0.142709\n",
      "122: Pretraining Loss:0.142589\n",
      "123: Pretraining Loss:0.142463\n",
      "124: Pretraining Loss:0.142361\n",
      "125: Pretraining Loss:0.142209\n",
      "126: Pretraining Loss:0.142079\n",
      "127: Pretraining Loss:0.142018\n",
      "128: Pretraining Loss:0.141881\n",
      "129: Pretraining Loss:0.141848\n",
      "130: Pretraining Loss:0.141702\n",
      "131: Pretraining Loss:0.141602\n",
      "132: Pretraining Loss:0.141502\n",
      "133: Pretraining Loss:0.141456\n",
      "134: Pretraining Loss:0.141223\n",
      "135: Pretraining Loss:0.141177\n",
      "136: Pretraining Loss:0.141080\n",
      "137: Pretraining Loss:0.141010\n",
      "138: Pretraining Loss:0.140847\n",
      "139: Pretraining Loss:0.140781\n",
      "140: Pretraining Loss:0.140716\n",
      "141: Pretraining Loss:0.140677\n",
      "142: Pretraining Loss:0.140505\n",
      "143: Pretraining Loss:0.140476\n",
      "144: Pretraining Loss:0.140333\n",
      "145: Pretraining Loss:0.140250\n",
      "146: Pretraining Loss:0.140158\n",
      "147: Pretraining Loss:0.140138\n",
      "148: Pretraining Loss:0.140014\n",
      "149: Pretraining Loss:0.140066\n",
      "150: Pretraining Loss:0.139779\n",
      "151: Pretraining Loss:0.139742\n",
      "152: Pretraining Loss:0.139660\n",
      "153: Pretraining Loss:0.139603\n",
      "154: Pretraining Loss:0.139508\n",
      "155: Pretraining Loss:0.139453\n",
      "156: Pretraining Loss:0.139378\n",
      "157: Pretraining Loss:0.139319\n",
      "158: Pretraining Loss:0.139217\n",
      "159: Pretraining Loss:0.139069\n",
      "160: Pretraining Loss:0.138962\n",
      "161: Pretraining Loss:0.138999\n",
      "162: Pretraining Loss:0.138875\n",
      "163: Pretraining Loss:0.138765\n",
      "164: Pretraining Loss:0.138723\n",
      "165: Pretraining Loss:0.138678\n",
      "166: Pretraining Loss:0.138553\n",
      "167: Pretraining Loss:0.138483\n",
      "168: Pretraining Loss:0.138358\n",
      "169: Pretraining Loss:0.138419\n",
      "170: Pretraining Loss:0.138276\n",
      "171: Pretraining Loss:0.138169\n",
      "172: Pretraining Loss:0.138119\n",
      "173: Pretraining Loss:0.138019\n",
      "174: Pretraining Loss:0.137932\n",
      "175: Pretraining Loss:0.137879\n",
      "176: Pretraining Loss:0.137869\n",
      "177: Pretraining Loss:0.137689\n",
      "178: Pretraining Loss:0.137715\n",
      "179: Pretraining Loss:0.137569\n",
      "180: Pretraining Loss:0.137513\n",
      "181: Pretraining Loss:0.137402\n",
      "182: Pretraining Loss:0.137376\n",
      "183: Pretraining Loss:0.137342\n",
      "184: Pretraining Loss:0.137257\n",
      "185: Pretraining Loss:0.137178\n",
      "186: Pretraining Loss:0.137121\n",
      "187: Pretraining Loss:0.137111\n",
      "188: Pretraining Loss:0.136956\n",
      "189: Pretraining Loss:0.136927\n",
      "190: Pretraining Loss:0.136935\n",
      "191: Pretraining Loss:0.136879\n",
      "192: Pretraining Loss:0.136721\n",
      "193: Pretraining Loss:0.136690\n",
      "194: Pretraining Loss:0.136595\n",
      "195: Pretraining Loss:0.136554\n",
      "196: Pretraining Loss:0.136524\n",
      "197: Pretraining Loss:0.136430\n",
      "198: Pretraining Loss:0.136373\n",
      "199: Pretraining Loss:0.136303\n",
      "200: Pretraining Loss:0.136220\n",
      "201: Pretraining Loss:0.136148\n",
      "202: Pretraining Loss:0.136196\n",
      "203: Pretraining Loss:0.136023\n",
      "204: Pretraining Loss:0.135993\n",
      "205: Pretraining Loss:0.135935\n",
      "206: Pretraining Loss:0.135843\n",
      "207: Pretraining Loss:0.135829\n",
      "208: Pretraining Loss:0.135820\n",
      "209: Pretraining Loss:0.135704\n",
      "210: Pretraining Loss:0.135587\n",
      "211: Pretraining Loss:0.135605\n",
      "212: Pretraining Loss:0.135462\n",
      "213: Pretraining Loss:0.135477\n",
      "214: Pretraining Loss:0.135425\n",
      "215: Pretraining Loss:0.135413\n",
      "216: Pretraining Loss:0.135293\n",
      "217: Pretraining Loss:0.135250\n",
      "218: Pretraining Loss:0.135190\n",
      "219: Pretraining Loss:0.135150\n",
      "220: Pretraining Loss:0.135091\n",
      "221: Pretraining Loss:0.135034\n",
      "222: Pretraining Loss:0.134951\n",
      "223: Pretraining Loss:0.135006\n",
      "224: Pretraining Loss:0.134883\n",
      "225: Pretraining Loss:0.134790\n",
      "226: Pretraining Loss:0.134789\n",
      "227: Pretraining Loss:0.134735\n",
      "228: Pretraining Loss:0.134607\n",
      "229: Pretraining Loss:0.134661\n",
      "230: Pretraining Loss:0.134583\n",
      "231: Pretraining Loss:0.134487\n",
      "232: Pretraining Loss:0.134432\n",
      "233: Pretraining Loss:0.134378\n",
      "234: Pretraining Loss:0.134441\n",
      "235: Pretraining Loss:0.134346\n",
      "236: Pretraining Loss:0.134248\n",
      "237: Pretraining Loss:0.134176\n",
      "238: Pretraining Loss:0.134193\n",
      "239: Pretraining Loss:0.134113\n",
      "240: Pretraining Loss:0.134056\n",
      "241: Pretraining Loss:0.134045\n",
      "242: Pretraining Loss:0.134035\n",
      "243: Pretraining Loss:0.134048\n",
      "244: Pretraining Loss:0.133901\n",
      "245: Pretraining Loss:0.133870\n",
      "246: Pretraining Loss:0.133810\n",
      "247: Pretraining Loss:0.133701\n",
      "248: Pretraining Loss:0.133746\n",
      "249: Pretraining Loss:0.133703\n",
      "Pretrain finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-training stage\n",
    "model.pretrain(x=count, raw_data=raw_data, pre_lr=args.pre_lr, pre_epoch=args.pre_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa99644",
   "metadata": {},
   "source": [
    "## Clustering stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96c42f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering initialization: K-means: ACC: 0.975607202680067, NMI: 0.8674018678751776, ARI:0.9407655633996225\n",
      "scMTO clustering...\n",
      "0 :ACC 0.9750837521, NMI 0.8657787203, ARI 0.9398070068\n",
      "0: Total Loss 0.2580, ZINB Loss 0.1317, OT Loss 0.3246, KL Loss 0.0829\n",
      "delta_label  0.0036641541038525964 < tol  0.005\n",
      "Reached tolerance threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "# Clustering stage\n",
    "z, w, h, pred, acc, nmi, ari = model.fit(x=count, y=y, raw_data=raw_data, lr=args.lr, train_epoch=args.train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df731eb2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aeb463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End: ACC 0.9758, NMI 0.8685, ARI 0.9409\n"
     ]
    }
   ],
   "source": [
    "# Cluster performance evaluated by ACC, NMI, ARI metrics\n",
    "if y is not None:\n",
    "    print(f'The End: ACC {acc:.4f}, NMI {nmi:.4f}, ARI {ari:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
